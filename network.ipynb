{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.05\n",
    "ITERATIONS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputPairsDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim):\n",
    "        # Generate pairs of indices, ensuring they match in your desired way\n",
    "        # For simplicity, using identity matrix pairs here as placeholders\n",
    "        self.inputs = [torch.eye(input_dim)[i].reshape(1, -1).squeeze(0) for i in range(input_dim)]\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx % self.input_dim], self.inputs[idx % self.input_dim], self.inputs[(idx + 1) % self.input_dim]\n",
    "\n",
    "\n",
    "class LayerLocalNetwork(nn.Module):\n",
    "    def __init__(self, bottom_dim, top_dim, num_layers=1, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layer = {\n",
    "                'bottom_up': nn.Parameter(torch.randn(bottom_dim, top_dim)),\n",
    "                'top_down': nn.Parameter(torch.randn(top_dim, top_dim)),\n",
    "                'recurrent': nn.Parameter(torch.randn(top_dim, top_dim))\n",
    "            }\n",
    "            self.layers.append(nn.ParameterDict(layer))\n",
    "            self.optimizers.append({\n",
    "                'bottom_up': optim.Adam([layer['bottom_up']], lr=LEARNING_RATE),\n",
    "                'top_down': optim.Adam([layer['top_down']], lr=LEARNING_RATE),\n",
    "                'recurrent': optim.Adam([layer['recurrent']], lr=LEARNING_RATE),\n",
    "            })\n",
    "\n",
    "        self.activations = [torch.zeros(batch_size, top_dim) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, bottom_input, top_input):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].zero_grad()\n",
    "            self.optimizers[i]['top_down'].zero_grad()\n",
    "            self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            bottom_up_act = torch.mm(bottom_input.detach(), layer['bottom_up']) if i == 0 else torch.mm(\n",
    "                self.activations[i-1], layer['bottom_up'])\n",
    "            top_down_act = torch.mm(top_input.detach(), layer['top_down']) if i == self.num_layers - 1 else torch.mm(\n",
    "                self.activations[i+1], layer['top_down'])\n",
    "            recurrent_act = torch.mm(self.activations[i], layer['recurrent'])\n",
    "\n",
    "            total_input = bottom_up_act + top_down_act + recurrent_act\n",
    "            self.activations[i] = F.leaky_relu(total_input)\n",
    "\n",
    "        loss = self.compute_energy()\n",
    "        loss.backward()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].step()\n",
    "            self.optimizers[i]['top_down'].step()\n",
    "            self.optimizers[i]['recurrent'].step()\n",
    "        \n",
    "        for i in range(0, len(self.activations)):\n",
    "            self.activations[i] = self.activations[i].detach()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_energy(self):\n",
    "        # Push energy down proportional to activations\n",
    "        running_sum = 0\n",
    "        for act in self.activations:\n",
    "            # pow the activations, average across neurons in layer, average across batches\n",
    "            running_sum += torch.mean(torch.mean(act.pow(2), dim=1), dim=0)\n",
    "        standard_loss = running_sum / len(self.activations)\n",
    "\n",
    "        # Hebbian loss computation: encourage variance at neuron level\n",
    "        hebbian_loss = 0\n",
    "        for act in self.activations:\n",
    "            hebbian_loss += self.generate_lpl_loss_hebbian(act)\n",
    "\n",
    "        # TODO: predictive and decorrelative losses\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = standard_loss + hebbian_loss  # Consider weighting factors if necessary\n",
    "        # total_loss = standard_loss\n",
    "        return total_loss\n",
    "\n",
    "    def generate_lpl_loss_hebbian(self, activations):\n",
    "        mean_act = torch.mean(activations, dim=0)\n",
    "        mean_subtracted = activations - mean_act\n",
    "        sigma_squared = torch.sum(mean_subtracted ** 2, dim=0) / (activations.shape[0] - 1)\n",
    "        loss = -torch.log(sigma_squared + 1e-10).sum() / sigma_squared.shape[0]\n",
    "        return loss\n",
    "\n",
    "dataset = InputPairsDataset(num_samples=100, input_dim=INPUT_DIM)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Example usage:\n",
    "model = LayerLocalNetwork(bottom_dim=INPUT_DIM, top_dim=INPUT_DIM, num_layers=NUM_LAYERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     for bottom_input, top_input, next_input in dataloader:\n",
    "#         for i in range(ITERATIONS):\n",
    "#             energy = model(bottom_input, top_input)\n",
    "#             # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "#             print(\"Energy:\", f\"{energy.item(): .2f}\")\n",
    "#         print(\"-----\")\n",
    "#     print()\n",
    "\n",
    "# print(\"======TRYING NEGATIVE SAMPLES======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Average Loss:  10497.498\n",
      "Average Loss:  84.995\n",
      "Average Loss:  2.853\n",
      "Average Loss:  2.407\n",
      "Average Loss:  2.647\n",
      "Average Loss:  2.337\n",
      "Average Loss:  2.771\n",
      "Average Loss:  2.855\n",
      "Average Loss:  2.562\n",
      "Average Loss:  1.706\n",
      "Average Loss:  2.602\n",
      "Average Loss:  2.432\n",
      "Average Loss:  1.860\n",
      "Average Loss:  2.386\n",
      "Average Loss:  2.354\n",
      "Average Loss:  2.220\n",
      "Average Loss:  2.328\n",
      "Average Loss:  2.185\n",
      "Average Loss:  1.965\n",
      "Average Loss:  1.647\n",
      "\n",
      "======TRYING NEGATIVE SAMPLES======\n",
      "Average Loss:  6.191\n",
      "======TRYING POSITIVE SAMPLE AGAIN======\n",
      "Epoch: 0\n",
      "Average Loss:  3.038\n",
      "Average Loss:  1.964\n",
      "Average Loss:  1.805\n",
      "Average Loss:  1.802\n",
      "Average Loss:  1.947\n",
      "Average Loss:  2.461\n",
      "Average Loss:  2.146\n",
      "Average Loss:  1.330\n",
      "Average Loss:  3.428\n",
      "Average Loss:  3.186\n",
      "Average Loss:  2.064\n",
      "Average Loss:  2.277\n",
      "Average Loss:  1.639\n",
      "Average Loss:  2.716\n",
      "Average Loss:  1.819\n",
      "Average Loss:  1.945\n",
      "Average Loss:  3.039\n",
      "Average Loss:  2.388\n",
      "Average Loss:  6.356\n",
      "Average Loss:  2.144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        layer_activations_queue = deque(maxlen=10)\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "\n",
    "            # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "            # layer_activations_queue.append(layer_activations)\n",
    "\n",
    "            # input_to_decoder = torch.stack(list(layer_activations_queue), dim=1)\n",
    "            # print(\"layer activations shape: \", layer_activations.shape)\n",
    "            # print(\"shape: \", input_to_decoder.shape)\n",
    "\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n",
    "\n",
    "print(\"======TRYING NEGATIVE SAMPLES======\")\n",
    "\n",
    "\n",
    "bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "top_input = torch.eye(10)[1].reshape(1, -1)    # One-hot vector for top input\n",
    "\n",
    "running_sum = 0\n",
    "for i in range(75):\n",
    "    loss = model(bottom_input, top_input)\n",
    "    running_sum += loss.item()\n",
    "    # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "print(\"Average Loss:\", f\"{running_sum / 75: .3f}\")\n",
    "\n",
    "\n",
    "print(\"======TRYING POSITIVE SAMPLE AGAIN======\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
