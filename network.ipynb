{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.05\n",
    "ITERATIONS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputPairsDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim):\n",
    "        # Generate pairs of indices, ensuring they match in your desired way\n",
    "        # For simplicity, using identity matrix pairs here as placeholders\n",
    "        self.inputs = [torch.eye(input_dim)[i].reshape(1, -1).squeeze(0) for i in range(input_dim)]\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx % self.input_dim], self.inputs[idx % self.input_dim], self.inputs[(idx + 1) % self.input_dim]\n",
    "\n",
    "\n",
    "class LayerLocalNetwork(nn.Module):\n",
    "    def __init__(self, bottom_dim, top_dim, num_layers=1, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layer = {\n",
    "                'bottom_up': nn.Parameter(torch.randn(bottom_dim, top_dim)),\n",
    "                'top_down': nn.Parameter(torch.randn(top_dim, top_dim)),\n",
    "                'recurrent': nn.Parameter(torch.randn(top_dim, top_dim))\n",
    "            }\n",
    "            self.layers.append(nn.ParameterDict(layer))\n",
    "            self.optimizers.append({\n",
    "                'bottom_up': optim.Adam([layer['bottom_up']], lr=LEARNING_RATE),\n",
    "                'top_down': optim.Adam([layer['top_down']], lr=LEARNING_RATE),\n",
    "                'recurrent': optim.Adam([layer['recurrent']], lr=LEARNING_RATE),\n",
    "            })\n",
    "\n",
    "        self.activations = [torch.zeros(batch_size, top_dim) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, bottom_input, top_input):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].zero_grad()\n",
    "            self.optimizers[i]['top_down'].zero_grad()\n",
    "            self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            bottom_up_act = torch.mm(bottom_input.detach(), layer['bottom_up']) if i == 0 else torch.mm(\n",
    "                self.activations[i-1], layer['bottom_up'])\n",
    "            top_down_act = torch.mm(top_input.detach(), layer['top_down']) if i == self.num_layers - 1 else torch.mm(\n",
    "                self.activations[i+1], layer['top_down'])\n",
    "            recurrent_act = torch.mm(self.activations[i], layer['recurrent'])\n",
    "\n",
    "            total_input = bottom_up_act + top_down_act + recurrent_act\n",
    "            self.activations[i] = F.leaky_relu(total_input)\n",
    "\n",
    "        loss = self.compute_energy()\n",
    "        loss.backward()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # self.optimizers[i]['bottom_up'].zero_grad()\n",
    "            # self.optimizers[i]['top_down'].zero_grad()\n",
    "            # self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "            self.optimizers[i]['bottom_up'].step()\n",
    "            self.optimizers[i]['top_down'].step()\n",
    "            self.optimizers[i]['recurrent'].step()\n",
    "        \n",
    "        for i in range(0, len(self.activations)):\n",
    "            self.activations[i] = self.activations[i].detach()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # def forward(self, bottom_input, top_input):\n",
    "    #     for i, layer in enumerate(self.layers):\n",
    "    #         self.optimizers[i]['bottom_up'].zero_grad()\n",
    "    #         self.optimizers[i]['top_down'].zero_grad()\n",
    "    #         self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "    #     for i, layer in enumerate(self.layers):\n",
    "    #         bottom_up_act = torch.mm(bottom_input.detach(), layer['bottom_up']) if i == 0 else torch.mm(\n",
    "    #             self.activations[i-1], layer['bottom_up'])\n",
    "    #         top_down_act = torch.mm(top_input.detach(), layer['top_down']) if i == self.num_layers - 1 else torch.mm(\n",
    "    #             self.activations[i+1], layer['top_down'])\n",
    "    #         recurrent_act = torch.mm(self.activations[i], layer['recurrent'])\n",
    "\n",
    "    #         total_input = bottom_up_act + top_down_act + recurrent_act\n",
    "    #         self.activations[i] = F.leaky_relu(total_input)\n",
    "\n",
    "    #     loss = self.compute_energy()\n",
    "    #     loss.backward()\n",
    "\n",
    "    #     for i, layer in enumerate(self.layers):\n",
    "    #         self.optimizers[i]['bottom_up'].step()\n",
    "    #         self.optimizers[i]['top_down'].step()\n",
    "    #         self.optimizers[i]['recurrent'].step()\n",
    "\n",
    "    #     for i in range(0, len(self.activations)): \n",
    "    #         self.activations[i] = self.activations[i].detach()\n",
    "\n",
    "    #     return loss\n",
    "\n",
    "    def compute_energy(self):\n",
    "        # Push energy down proportional to activations\n",
    "        running_sum = 0\n",
    "        for act in self.activations:\n",
    "            running_sum += torch.mean(act.pow(2))\n",
    "        standard_loss = running_sum / len(self.activations)\n",
    "\n",
    "        # Hebbian loss computation: encourage variance at neuron level\n",
    "        hebbian_loss = 0\n",
    "        for act in self.activations:\n",
    "            hebbian_loss += self.generate_lpl_loss_hebbian(act)\n",
    "\n",
    "        # TODO: predictive and decorrelative losses\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = standard_loss + hebbian_loss  # Consider weighting factors if necessary\n",
    "        # total_loss = standard_loss\n",
    "        return total_loss\n",
    "\n",
    "    def generate_lpl_loss_hebbian(self, activations):\n",
    "        mean_act = torch.mean(activations, dim=0)\n",
    "        mean_subtracted = activations - mean_act\n",
    "        sigma_squared = torch.sum(mean_subtracted ** 2, dim=0) / (activations.shape[0] - 1)\n",
    "        loss = -torch.log(sigma_squared + 1e-10).sum() / sigma_squared.shape[0]\n",
    "        return loss\n",
    "\n",
    "dataset = InputPairsDataset(num_samples=100, input_dim=INPUT_DIM)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Example usage:\n",
    "model = LayerLocalNetwork(bottom_dim=INPUT_DIM, top_dim=INPUT_DIM, num_layers=NUM_LAYERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Energy:  1.89\n",
      "Energy:  3.06\n",
      "Energy:  20.23\n",
      "Energy:  91.40\n",
      "Energy:  346.60\n",
      "Energy:  1330.58\n",
      "Energy:  4793.13\n",
      "Energy:  16945.21\n",
      "Energy:  42499.76\n",
      "Energy:  62471.09\n",
      "Energy:  56569.56\n",
      "Energy:  68043.00\n",
      "Energy:  74493.95\n",
      "Energy:  66915.76\n",
      "Energy:  85640.95\n",
      "Energy:  73890.50\n",
      "Energy:  89361.64\n",
      "Energy:  54355.34\n",
      "Energy:  145524.22\n",
      "Energy:  57544.78\n",
      "Energy:  72399.02\n",
      "Energy:  41723.98\n",
      "Energy:  47667.84\n",
      "Energy:  6036.65\n",
      "Energy:  8914.87\n",
      "Energy:  11717.92\n",
      "Energy:  3527.74\n",
      "Energy:  5118.65\n",
      "Energy:  739.25\n",
      "Energy:  785.26\n",
      "Energy:  420.55\n",
      "Energy:  289.55\n",
      "Energy:  715.00\n",
      "Energy:  110.45\n",
      "Energy:  70.94\n",
      "Energy:  34.66\n",
      "Energy:  70.66\n",
      "Energy:  14.56\n",
      "Energy:  16.37\n",
      "Energy:  13.75\n",
      "Energy:  8.87\n",
      "Energy:  9.45\n",
      "Energy:  14.65\n",
      "Energy:  5.70\n",
      "Energy:  4.53\n",
      "Energy:  6.19\n",
      "Energy:  5.98\n",
      "Energy:  3.95\n",
      "Energy:  3.72\n",
      "Energy:  4.94\n",
      "-----\n",
      "Energy:  6.43\n",
      "Energy:  2.38\n",
      "Energy:  1.93\n",
      "Energy:  2.03\n",
      "Energy:  3.83\n",
      "Energy:  2.58\n",
      "Energy:  3.14\n",
      "Energy:  4.26\n",
      "Energy:  3.41\n",
      "Energy:  3.46\n",
      "Energy:  3.21\n",
      "Energy:  2.42\n",
      "Energy:  2.41\n",
      "Energy:  4.00\n",
      "Energy:  2.90\n",
      "Energy:  5.13\n",
      "Energy:  2.42\n",
      "Energy:  2.07\n",
      "Energy:  2.27\n",
      "Energy:  2.29\n",
      "Energy:  3.91\n",
      "Energy:  3.87\n",
      "Energy:  3.31\n",
      "Energy:  2.62\n",
      "Energy:  4.82\n",
      "Energy:  2.10\n",
      "Energy:  3.94\n",
      "Energy:  3.25\n",
      "Energy:  2.03\n",
      "Energy:  2.24\n",
      "Energy:  3.27\n",
      "Energy:  3.17\n",
      "Energy:  3.60\n",
      "Energy:  3.14\n",
      "Energy:  2.47\n",
      "Energy:  2.62\n",
      "Energy:  2.57\n",
      "Energy:  2.83\n",
      "Energy:  2.89\n",
      "Energy:  4.00\n",
      "Energy:  2.86\n",
      "Energy:  2.79\n",
      "Energy:  3.76\n",
      "Energy:  2.49\n",
      "Energy:  2.74\n",
      "Energy:  3.67\n",
      "Energy:  3.29\n",
      "Energy:  2.87\n",
      "Energy:  3.73\n",
      "Energy:  3.35\n",
      "-----\n",
      "Energy:  4.05\n",
      "Energy:  7.67\n",
      "Energy:  8.83\n",
      "Energy:  3.64\n",
      "Energy:  7.46\n",
      "Energy:  10.01\n",
      "Energy:  10.28\n",
      "Energy:  2.70\n",
      "Energy:  1.75\n",
      "Energy:  4.39\n",
      "Energy:  4.28\n",
      "Energy:  4.00\n",
      "Energy:  3.35\n",
      "Energy:  4.57\n",
      "Energy:  2.43\n",
      "Energy:  4.14\n",
      "Energy:  3.37\n",
      "Energy:  4.34\n",
      "Energy:  3.10\n",
      "Energy:  4.83\n",
      "Energy:  2.56\n",
      "Energy:  4.43\n",
      "Energy:  3.29\n",
      "Energy:  4.23\n",
      "Energy:  2.93\n",
      "Energy:  4.88\n",
      "Energy:  1.99\n",
      "Energy:  4.30\n",
      "Energy:  3.25\n",
      "Energy:  3.11\n",
      "Energy:  3.07\n",
      "Energy:  4.85\n",
      "Energy:  2.40\n",
      "Energy:  3.96\n",
      "Energy:  1.18\n",
      "Energy:  3.66\n",
      "Energy:  3.21\n",
      "Energy:  4.29\n",
      "Energy:  2.02\n",
      "Energy:  4.25\n",
      "Energy:  3.72\n",
      "Energy:  4.68\n",
      "Energy:  5.10\n",
      "Energy:  5.60\n",
      "Energy:  5.82\n",
      "Energy:  6.38\n",
      "Energy:  3.06\n",
      "Energy:  4.93\n",
      "Energy:  3.93\n",
      "Energy:  6.56\n",
      "-----\n",
      "Energy:  4.13\n",
      "Energy:  4.49\n",
      "Energy:  7.33\n",
      "Energy:  8.57\n",
      "Energy:  10.65\n",
      "Energy:  10.37\n",
      "Energy:  3.67\n",
      "Energy:  3.79\n",
      "Energy:  3.43\n",
      "Energy:  3.44\n",
      "Energy:  4.37\n",
      "Energy:  5.05\n",
      "Energy:  5.07\n",
      "Energy:  4.87\n",
      "Energy:  3.91\n",
      "Energy:  3.87\n",
      "Energy:  3.85\n",
      "Energy:  3.55\n",
      "Energy:  2.89\n",
      "Energy:  4.72\n",
      "Energy:  3.94\n",
      "Energy:  4.30\n",
      "Energy:  3.34\n",
      "Energy:  3.05\n",
      "Energy:  2.65\n",
      "Energy:  2.60\n",
      "Energy:  2.84\n",
      "Energy:  3.60\n",
      "Energy:  3.16\n",
      "Energy:  3.06\n",
      "Energy:  3.06\n",
      "Energy:  3.03\n",
      "Energy:  2.89\n",
      "Energy:  2.60\n",
      "Energy:  2.80\n",
      "Energy:  2.82\n",
      "Energy:  2.90\n",
      "Energy:  2.95\n",
      "Energy:  2.95\n",
      "Energy:  3.03\n",
      "Energy:  2.94\n",
      "Energy:  2.73\n",
      "Energy:  2.62\n",
      "Energy:  2.51\n",
      "Energy:  2.43\n",
      "Energy:  2.42\n",
      "Energy:  2.37\n",
      "Energy:  2.42\n",
      "Energy:  2.34\n",
      "Energy:  2.30\n",
      "-----\n",
      "Energy:  3.51\n",
      "Energy:  3.09\n",
      "Energy:  4.33\n",
      "Energy:  3.40\n",
      "Energy:  4.49\n",
      "Energy:  3.42\n",
      "Energy:  3.44\n",
      "Energy:  3.57\n",
      "Energy:  4.96\n",
      "Energy:  3.08\n",
      "Energy:  4.94\n",
      "Energy:  2.83\n",
      "Energy:  3.66\n",
      "Energy:  3.56\n",
      "Energy:  3.17\n",
      "Energy:  3.45\n",
      "Energy:  4.08\n",
      "Energy:  3.07\n",
      "Energy:  5.44\n",
      "Energy:  3.27\n",
      "Energy:  3.89\n",
      "Energy:  2.84\n",
      "Energy:  2.80\n",
      "Energy:  3.58\n",
      "Energy:  2.77\n",
      "Energy:  3.01\n",
      "Energy:  3.45\n",
      "Energy:  3.19\n",
      "Energy:  3.18\n",
      "Energy:  1.99\n",
      "Energy:  3.32\n",
      "Energy:  2.89\n",
      "Energy:  3.20\n",
      "Energy:  2.82\n",
      "Energy:  2.65\n",
      "Energy:  2.36\n",
      "Energy:  3.81\n",
      "Energy:  2.95\n",
      "Energy:  2.67\n",
      "Energy:  3.35\n",
      "Energy:  3.00\n",
      "Energy:  3.16\n",
      "Energy:  3.46\n",
      "Energy:  3.10\n",
      "Energy:  3.60\n",
      "Energy:  3.00\n",
      "Energy:  3.42\n",
      "Energy:  4.07\n",
      "Energy:  4.36\n",
      "Energy:  4.50\n",
      "-----\n",
      "Energy:  3.74\n",
      "Energy:  2.59\n",
      "Energy:  1.56\n",
      "Energy:  2.05\n",
      "Energy:  1.45\n",
      "Energy:  1.44\n",
      "Energy:  1.76\n",
      "Energy:  2.15\n",
      "Energy:  1.80\n",
      "Energy:  1.37\n",
      "Energy:  1.73\n",
      "Energy:  1.72\n",
      "Energy:  1.88\n",
      "Energy:  1.88\n",
      "Energy:  1.29\n",
      "Energy:  1.53\n",
      "Energy:  1.33\n",
      "Energy:  1.11\n",
      "Energy:  1.18\n",
      "Energy:  1.29\n",
      "Energy:  1.37\n",
      "Energy:  1.10\n",
      "Energy:  1.01\n",
      "Energy:  1.10\n",
      "Energy:  1.31\n",
      "Energy:  1.21\n",
      "Energy:  1.11\n",
      "Energy:  1.19\n",
      "Energy:  1.41\n",
      "Energy:  1.29\n",
      "Energy:  1.52\n",
      "Energy:  1.43\n",
      "Energy:  1.25\n",
      "Energy:  1.57\n",
      "Energy:  1.39\n",
      "Energy:  1.75\n",
      "Energy:  1.57\n",
      "Energy:  1.10\n",
      "Energy:  1.24\n",
      "Energy:  1.23\n",
      "Energy:  1.11\n",
      "Energy:  1.21\n",
      "Energy:  1.10\n",
      "Energy:  1.11\n",
      "Energy:  1.13\n",
      "Energy:  1.08\n",
      "Energy:  1.06\n",
      "Energy:  1.05\n",
      "Energy:  1.07\n",
      "Energy:  1.07\n",
      "-----\n",
      "Energy:  4.01\n",
      "Energy:  3.38\n",
      "Energy:  2.62\n",
      "Energy:  4.38\n",
      "Energy:  1.70\n",
      "Energy:  4.12\n",
      "Energy:  3.28\n",
      "Energy:  4.24\n",
      "Energy:  3.99\n",
      "Energy:  3.44\n",
      "Energy:  4.53\n",
      "Energy:  3.52\n",
      "Energy:  4.76\n",
      "Energy:  3.32\n",
      "Energy:  3.37\n",
      "Energy:  3.50\n",
      "Energy:  3.25\n",
      "Energy:  5.14\n",
      "Energy:  3.05\n",
      "Energy:  5.48\n",
      "Energy:  2.67\n",
      "Energy:  3.58\n",
      "Energy:  5.79\n",
      "Energy:  3.01\n",
      "Energy:  3.23\n",
      "Energy:  3.85\n",
      "Energy:  5.41\n",
      "Energy:  5.34\n",
      "Energy:  4.93\n",
      "Energy:  4.46\n",
      "Energy:  2.56\n",
      "Energy:  3.12\n",
      "Energy:  3.69\n",
      "Energy:  2.86\n",
      "Energy:  4.79\n",
      "Energy:  2.07\n",
      "Energy:  3.71\n",
      "Energy:  2.60\n",
      "Energy:  4.52\n",
      "Energy:  3.01\n",
      "Energy:  3.64\n",
      "Energy:  3.42\n",
      "Energy:  3.22\n",
      "Energy:  2.62\n",
      "Energy:  3.12\n",
      "Energy:  3.32\n",
      "Energy:  3.35\n",
      "Energy:  2.07\n",
      "Energy:  3.19\n",
      "Energy:  2.07\n",
      "-----\n",
      "Energy:  2.95\n",
      "Energy:  3.27\n",
      "Energy:  2.23\n",
      "Energy:  2.44\n",
      "Energy:  2.85\n",
      "Energy:  2.33\n",
      "Energy:  3.75\n",
      "Energy:  2.38\n",
      "Energy:  2.27\n",
      "Energy:  2.24\n",
      "Energy:  2.05\n",
      "Energy:  3.72\n",
      "Energy:  3.04\n",
      "Energy:  2.36\n",
      "Energy:  2.90\n",
      "Energy:  2.17\n",
      "Energy:  2.23\n",
      "Energy:  2.61\n",
      "Energy:  2.38\n",
      "Energy:  2.38\n",
      "Energy:  1.95\n",
      "Energy:  3.60\n",
      "Energy:  2.08\n",
      "Energy:  2.23\n",
      "Energy:  1.93\n",
      "Energy:  2.12\n",
      "Energy:  3.56\n",
      "Energy:  2.83\n",
      "Energy:  2.51\n",
      "Energy:  1.96\n",
      "Energy:  1.93\n",
      "Energy:  3.02\n",
      "Energy:  2.48\n",
      "Energy:  2.21\n",
      "Energy:  2.03\n",
      "Energy:  2.06\n",
      "Energy:  2.33\n",
      "Energy:  2.22\n",
      "Energy:  1.90\n",
      "Energy:  1.87\n",
      "Energy:  2.11\n",
      "Energy:  2.18\n",
      "Energy:  2.24\n",
      "Energy:  1.66\n",
      "Energy:  1.78\n",
      "Energy:  1.92\n",
      "Energy:  2.01\n",
      "Energy:  2.32\n",
      "Energy:  2.20\n",
      "Energy:  2.18\n",
      "-----\n",
      "Energy:  2.72\n",
      "Energy:  2.76\n",
      "Energy:  2.71\n",
      "Energy:  1.38\n",
      "Energy:  3.42\n",
      "Energy:  1.83\n",
      "Energy:  2.94\n",
      "Energy:  1.32\n",
      "Energy:  2.72\n",
      "Energy:  1.87\n",
      "Energy:  2.69\n",
      "Energy:  1.31\n",
      "Energy:  2.82\n",
      "Energy:  1.69\n",
      "Energy:  3.01\n",
      "Energy:  1.96\n",
      "Energy:  2.99\n",
      "Energy:  1.72\n",
      "Energy:  2.92\n",
      "Energy:  2.02\n",
      "Energy:  2.85\n",
      "Energy:  1.55\n",
      "Energy:  2.86\n",
      "Energy:  1.79\n",
      "Energy:  2.17\n",
      "Energy:  2.79\n",
      "Energy:  2.66\n",
      "Energy:  1.67\n",
      "Energy:  2.45\n",
      "Energy:  1.96\n",
      "Energy:  2.79\n",
      "Energy:  1.37\n",
      "Energy:  2.85\n",
      "Energy:  1.78\n",
      "Energy:  2.13\n",
      "Energy:  2.49\n",
      "Energy:  2.37\n",
      "Energy:  1.89\n",
      "Energy:  2.86\n",
      "Energy:  1.69\n",
      "Energy:  3.02\n",
      "Energy:  1.19\n",
      "Energy:  2.99\n",
      "Energy:  1.34\n",
      "Energy:  2.20\n",
      "Energy:  2.07\n",
      "Energy:  2.59\n",
      "Energy:  1.27\n",
      "Energy:  2.73\n",
      "Energy:  1.20\n",
      "-----\n",
      "Energy:  3.12\n",
      "Energy:  2.98\n",
      "Energy:  2.87\n",
      "Energy:  2.99\n",
      "Energy:  3.05\n",
      "Energy:  2.55\n",
      "Energy:  2.80\n",
      "Energy:  2.59\n",
      "Energy:  2.17\n",
      "Energy:  1.80\n",
      "Energy:  2.37\n",
      "Energy:  2.37\n",
      "Energy:  2.51\n",
      "Energy:  2.28\n",
      "Energy:  2.39\n",
      "Energy:  2.00\n",
      "Energy:  2.78\n",
      "Energy:  2.09\n",
      "Energy:  2.50\n",
      "Energy:  2.09\n",
      "Energy:  2.31\n",
      "Energy:  2.30\n",
      "Energy:  2.28\n",
      "Energy:  1.84\n",
      "Energy:  1.76\n",
      "Energy:  1.66\n",
      "Energy:  2.08\n",
      "Energy:  1.97\n",
      "Energy:  2.45\n",
      "Energy:  1.93\n",
      "Energy:  2.62\n",
      "Energy:  2.16\n",
      "Energy:  2.30\n",
      "Energy:  1.95\n",
      "Energy:  2.30\n",
      "Energy:  2.17\n",
      "Energy:  2.27\n",
      "Energy:  2.93\n",
      "Energy:  1.19\n",
      "Energy:  1.82\n",
      "Energy:  2.09\n",
      "Energy:  1.90\n",
      "Energy:  1.97\n",
      "Energy:  1.88\n",
      "Energy:  2.83\n",
      "Energy:  2.22\n",
      "Energy:  2.43\n",
      "Energy:  2.15\n",
      "Energy:  2.53\n",
      "Energy:  2.38\n",
      "-----\n",
      "Energy:  1.17\n",
      "Energy:  3.16\n",
      "Energy:  2.10\n",
      "Energy:  3.23\n",
      "Energy:  1.64\n",
      "Energy:  3.06\n",
      "Energy:  2.19\n",
      "Energy:  3.40\n",
      "Energy:  1.60\n",
      "Energy:  2.19\n",
      "Energy:  1.91\n",
      "Energy:  3.19\n",
      "Energy:  1.38\n",
      "Energy:  2.27\n",
      "Energy:  1.93\n",
      "Energy:  3.14\n",
      "Energy:  1.32\n",
      "Energy:  2.23\n",
      "Energy:  1.91\n",
      "Energy:  3.14\n",
      "Energy:  1.34\n",
      "Energy:  2.24\n",
      "Energy:  1.91\n",
      "Energy:  3.17\n",
      "Energy:  1.35\n",
      "Energy:  2.31\n",
      "Energy:  1.88\n",
      "Energy:  3.23\n",
      "Energy:  1.29\n",
      "Energy:  2.41\n",
      "Energy:  1.80\n",
      "Energy:  3.25\n",
      "Energy:  1.20\n",
      "Energy:  2.32\n",
      "Energy:  1.70\n",
      "Energy:  3.07\n",
      "Energy:  1.13\n",
      "Energy:  2.14\n",
      "Energy:  1.64\n",
      "Energy:  2.91\n",
      "Energy:  1.09\n",
      "Energy:  2.03\n",
      "Energy:  1.60\n",
      "Energy:  2.81\n",
      "Energy:  1.07\n",
      "Energy:  1.95\n",
      "Energy:  1.58\n",
      "Energy:  2.75\n",
      "Energy:  1.06\n",
      "Energy:  1.91\n",
      "-----\n",
      "Energy:  2.83\n",
      "Energy:  3.41\n",
      "Energy:  3.60\n",
      "Energy:  3.80\n",
      "Energy:  2.73\n",
      "Energy:  3.59\n",
      "Energy:  2.98\n",
      "Energy:  3.38\n",
      "Energy:  3.59\n",
      "Energy:  3.39\n",
      "Energy:  4.02\n",
      "Energy:  2.82\n",
      "Energy:  3.44\n",
      "Energy:  2.76\n",
      "Energy:  3.40\n",
      "Energy:  2.92\n",
      "Energy:  3.47\n",
      "Energy:  3.30\n",
      "Energy:  3.15\n",
      "Energy:  3.13\n",
      "Energy:  2.96\n",
      "Energy:  3.22\n",
      "Energy:  2.92\n",
      "Energy:  3.14\n",
      "Energy:  2.67\n",
      "Energy:  3.18\n",
      "Energy:  2.70\n",
      "Energy:  3.27\n",
      "Energy:  2.57\n",
      "Energy:  3.27\n",
      "Energy:  2.61\n",
      "Energy:  3.23\n",
      "Energy:  2.43\n",
      "Energy:  3.24\n",
      "Energy:  2.42\n",
      "Energy:  3.19\n",
      "Energy:  2.34\n",
      "Energy:  3.04\n",
      "Energy:  2.32\n",
      "Energy:  2.63\n",
      "Energy:  2.30\n",
      "Energy:  2.47\n",
      "Energy:  2.24\n",
      "Energy:  2.52\n",
      "Energy:  2.28\n",
      "Energy:  2.40\n",
      "Energy:  2.24\n",
      "Energy:  2.49\n",
      "Energy:  2.26\n",
      "Energy:  2.40\n",
      "-----\n",
      "Energy:  3.77\n",
      "Energy:  4.59\n",
      "Energy:  4.75\n",
      "Energy:  2.71\n",
      "Energy:  3.31\n",
      "Energy:  2.93\n",
      "Energy:  2.54\n",
      "Energy:  2.53\n",
      "Energy:  3.35\n",
      "Energy:  3.29\n",
      "Energy:  3.06\n",
      "Energy:  3.29\n",
      "Energy:  2.55\n",
      "Energy:  3.75\n",
      "Energy:  3.07\n",
      "Energy:  2.90\n",
      "Energy:  2.77\n",
      "Energy:  3.73\n",
      "Energy:  4.77\n",
      "Energy:  2.50\n",
      "Energy:  2.53\n",
      "Energy:  3.94\n",
      "Energy:  4.74\n",
      "Energy:  4.94\n",
      "Energy:  2.70\n",
      "Energy:  3.37\n",
      "Energy:  3.04\n",
      "Energy:  4.60\n",
      "Energy:  4.82\n",
      "Energy:  2.68\n",
      "Energy:  2.18\n",
      "Energy:  2.62\n",
      "Energy:  2.38\n",
      "Energy:  3.11\n",
      "Energy:  2.49\n",
      "Energy:  2.64\n",
      "Energy:  1.37\n",
      "Energy:  2.54\n",
      "Energy:  2.69\n",
      "Energy:  2.60\n",
      "Energy:  1.34\n",
      "Energy:  2.57\n",
      "Energy:  2.64\n",
      "Energy:  2.60\n",
      "Energy:  1.33\n",
      "Energy:  2.56\n",
      "Energy:  2.57\n",
      "Energy:  2.62\n",
      "Energy:  1.34\n",
      "Energy:  2.52\n",
      "-----\n",
      "Energy:  4.46\n",
      "Energy:  3.05\n",
      "Energy:  3.20\n",
      "Energy:  2.69\n",
      "Energy:  2.82\n",
      "Energy:  4.07\n",
      "Energy:  2.94\n",
      "Energy:  3.70\n",
      "Energy:  2.94\n",
      "Energy:  2.36\n",
      "Energy:  3.81\n",
      "Energy:  2.88\n",
      "Energy:  3.41\n",
      "Energy:  2.36\n",
      "Energy:  3.27\n",
      "Energy:  3.23\n",
      "Energy:  2.57\n",
      "Energy:  2.86\n",
      "Energy:  2.15\n",
      "Energy:  3.29\n",
      "Energy:  2.17\n",
      "Energy:  2.88\n",
      "Energy:  2.13\n",
      "Energy:  1.91\n",
      "Energy:  2.15\n",
      "Energy:  2.55\n",
      "Energy:  3.84\n",
      "Energy:  2.14\n",
      "Energy:  3.06\n",
      "Energy:  3.53\n",
      "Energy:  3.58\n",
      "Energy:  2.46\n",
      "Energy:  3.95\n",
      "Energy:  5.06\n",
      "Energy:  4.60\n",
      "Energy:  3.36\n",
      "Energy:  3.76\n",
      "Energy:  6.81\n",
      "Energy:  8.31\n",
      "Energy:  2.95\n",
      "Energy:  4.01\n",
      "Energy:  5.46\n",
      "Energy:  6.88\n",
      "Energy:  3.45\n",
      "Energy:  2.56\n",
      "Energy:  4.20\n",
      "Energy:  5.59\n",
      "Energy:  4.69\n",
      "Energy:  3.31\n",
      "Energy:  4.25\n",
      "-----\n",
      "Energy:  5.90\n",
      "Energy:  7.07\n",
      "Energy:  3.83\n",
      "Energy:  2.77\n",
      "Energy:  3.76\n",
      "Energy:  4.55\n",
      "Energy:  2.73\n",
      "Energy:  2.70\n",
      "Energy:  3.42\n",
      "Energy:  4.12\n",
      "Energy:  2.96\n",
      "Energy:  2.56\n",
      "Energy:  2.95\n",
      "Energy:  3.91\n",
      "Energy:  2.64\n",
      "Energy:  2.22\n",
      "Energy:  2.36\n",
      "Energy:  3.44\n",
      "Energy:  3.25\n",
      "Energy:  2.36\n",
      "Energy:  2.28\n",
      "Energy:  2.22\n",
      "Energy:  3.08\n",
      "Energy:  1.77\n",
      "Energy:  1.63\n",
      "Energy:  2.66\n",
      "Energy:  2.90\n",
      "Energy:  2.93\n",
      "Energy:  1.37\n",
      "Energy:  2.17\n",
      "Energy:  1.95\n",
      "Energy:  2.13\n",
      "Energy:  2.14\n",
      "Energy:  1.67\n",
      "Energy:  2.02\n",
      "Energy:  2.00\n",
      "Energy:  1.96\n",
      "Energy:  1.76\n",
      "Energy:  1.64\n",
      "Energy:  1.99\n",
      "Energy:  2.19\n",
      "Energy:  2.54\n",
      "Energy:  1.70\n",
      "Energy:  1.90\n",
      "Energy:  1.73\n",
      "Energy:  1.78\n",
      "Energy:  1.54\n",
      "Energy:  1.76\n",
      "Energy:  1.94\n",
      "Energy:  1.56\n",
      "-----\n",
      "Energy:  3.80\n",
      "Energy:  2.81\n",
      "Energy:  3.03\n",
      "Energy:  3.85\n",
      "Energy:  3.36\n",
      "Energy:  3.91\n",
      "Energy:  3.36\n",
      "Energy:  3.03\n",
      "Energy:  2.67\n",
      "Energy:  3.00\n",
      "Energy:  2.26\n",
      "Energy:  3.69\n",
      "Energy:  2.26\n",
      "Energy:  3.58\n",
      "Energy:  2.55\n",
      "Energy:  3.34\n",
      "Energy:  3.36\n",
      "Energy:  3.83\n",
      "Energy:  3.50\n",
      "Energy:  2.68\n",
      "Energy:  2.79\n",
      "Energy:  2.58\n",
      "Energy:  2.43\n",
      "Energy:  2.56\n",
      "Energy:  2.56\n",
      "Energy:  2.31\n",
      "Energy:  2.55\n",
      "Energy:  2.35\n",
      "Energy:  2.32\n",
      "Energy:  2.33\n",
      "Energy:  2.57\n",
      "Energy:  2.53\n",
      "Energy:  2.75\n",
      "Energy:  2.32\n",
      "Energy:  2.20\n",
      "Energy:  2.26\n",
      "Energy:  2.18\n",
      "Energy:  2.27\n",
      "Energy:  2.33\n",
      "Energy:  2.65\n",
      "Energy:  2.83\n",
      "Energy:  2.94\n",
      "Energy:  2.71\n",
      "Energy:  2.82\n",
      "Energy:  2.62\n",
      "Energy:  2.62\n",
      "Energy:  2.38\n",
      "Energy:  2.24\n",
      "Energy:  2.22\n",
      "Energy:  2.12\n",
      "-----\n",
      "Energy:  4.74\n",
      "Energy:  3.51\n",
      "Energy:  3.77\n",
      "Energy:  1.80\n",
      "Energy:  2.61\n",
      "Energy:  2.31\n",
      "Energy:  1.77\n",
      "Energy:  1.69\n",
      "Energy:  1.72\n",
      "Energy:  1.79\n",
      "Energy:  1.67\n",
      "Energy:  1.63\n",
      "Energy:  1.62\n",
      "Energy:  1.46\n",
      "Energy:  1.30\n",
      "Energy:  1.29\n",
      "Energy:  1.29\n",
      "Energy:  1.49\n",
      "Energy:  1.57\n",
      "Energy:  1.53\n",
      "Energy:  1.45\n",
      "Energy:  1.27\n",
      "Energy:  1.16\n",
      "Energy:  1.36\n",
      "Energy:  1.27\n",
      "Energy:  1.17\n",
      "Energy:  1.12\n",
      "Energy:  1.21\n",
      "Energy:  1.13\n",
      "Energy:  1.11\n",
      "Energy:  1.10\n",
      "Energy:  1.18\n",
      "Energy:  1.17\n",
      "Energy:  1.16\n",
      "Energy:  1.14\n",
      "Energy:  1.10\n",
      "Energy:  1.09\n",
      "Energy:  1.07\n",
      "Energy:  1.07\n",
      "Energy:  1.07\n",
      "Energy:  1.07\n",
      "Energy:  1.07\n",
      "Energy:  1.07\n",
      "Energy:  1.08\n",
      "Energy:  1.08\n",
      "Energy:  1.08\n",
      "Energy:  1.09\n",
      "Energy:  1.08\n",
      "Energy:  1.10\n",
      "Energy:  1.09\n",
      "-----\n",
      "Energy:  2.59\n",
      "Energy:  4.24\n",
      "Energy:  1.68\n",
      "Energy:  4.34\n",
      "Energy:  1.88\n",
      "Energy:  4.54\n",
      "Energy:  2.38\n",
      "Energy:  4.96\n",
      "Energy:  1.86\n",
      "Energy:  4.56\n",
      "Energy:  2.02\n",
      "Energy:  4.92\n",
      "Energy:  1.88\n",
      "Energy:  4.94\n",
      "Energy:  1.90\n",
      "Energy:  4.90\n",
      "Energy:  1.92\n",
      "Energy:  4.86\n",
      "Energy:  1.94\n",
      "Energy:  4.83\n",
      "Energy:  1.96\n",
      "Energy:  4.80\n",
      "Energy:  1.98\n",
      "Energy:  4.77\n",
      "Energy:  2.00\n",
      "Energy:  4.75\n",
      "Energy:  2.01\n",
      "Energy:  4.73\n",
      "Energy:  2.03\n",
      "Energy:  4.72\n",
      "Energy:  2.04\n",
      "Energy:  4.70\n",
      "Energy:  2.05\n",
      "Energy:  4.69\n",
      "Energy:  2.05\n",
      "Energy:  4.67\n",
      "Energy:  2.05\n",
      "Energy:  4.66\n",
      "Energy:  2.05\n",
      "Energy:  4.59\n",
      "Energy:  2.05\n",
      "Energy:  4.39\n",
      "Energy:  2.04\n",
      "Energy:  4.32\n",
      "Energy:  2.04\n",
      "Energy:  4.23\n",
      "Energy:  2.04\n",
      "Energy:  4.18\n",
      "Energy:  2.03\n",
      "Energy:  4.11\n",
      "-----\n",
      "Energy:  2.75\n",
      "Energy:  3.14\n",
      "Energy:  2.34\n",
      "Energy:  4.09\n",
      "Energy:  1.59\n",
      "Energy:  3.89\n",
      "Energy:  1.63\n",
      "Energy:  3.84\n",
      "Energy:  1.72\n",
      "Energy:  3.83\n",
      "Energy:  1.78\n",
      "Energy:  3.83\n",
      "Energy:  1.81\n",
      "Energy:  3.83\n",
      "Energy:  1.84\n",
      "Energy:  3.83\n",
      "Energy:  1.85\n",
      "Energy:  3.82\n",
      "Energy:  1.87\n",
      "Energy:  3.80\n",
      "Energy:  1.88\n",
      "Energy:  3.78\n",
      "Energy:  1.86\n",
      "Energy:  3.76\n",
      "Energy:  1.82\n",
      "Energy:  3.74\n",
      "Energy:  1.79\n",
      "Energy:  3.73\n",
      "Energy:  1.76\n",
      "Energy:  3.73\n",
      "Energy:  1.73\n",
      "Energy:  3.67\n",
      "Energy:  1.71\n",
      "Energy:  3.40\n",
      "Energy:  1.77\n",
      "Energy:  3.20\n",
      "Energy:  2.09\n",
      "Energy:  3.18\n",
      "Energy:  2.06\n",
      "Energy:  3.13\n",
      "Energy:  2.04\n",
      "Energy:  3.07\n",
      "Energy:  2.01\n",
      "Energy:  3.06\n",
      "Energy:  2.00\n",
      "Energy:  3.09\n",
      "Energy:  1.98\n",
      "Energy:  3.08\n",
      "Energy:  1.96\n",
      "Energy:  3.14\n",
      "-----\n",
      "Energy:  2.68\n",
      "Energy:  2.55\n",
      "Energy:  2.25\n",
      "Energy:  2.74\n",
      "Energy:  2.06\n",
      "Energy:  2.74\n",
      "Energy:  1.80\n",
      "Energy:  2.60\n",
      "Energy:  1.63\n",
      "Energy:  2.89\n",
      "Energy:  2.42\n",
      "Energy:  5.26\n",
      "Energy:  7.25\n",
      "Energy:  10.71\n",
      "Energy:  8.70\n",
      "Energy:  7.09\n",
      "Energy:  6.53\n",
      "Energy:  8.58\n",
      "Energy:  9.93\n",
      "Energy:  9.09\n",
      "Energy:  8.81\n",
      "Energy:  8.00\n",
      "Energy:  9.54\n",
      "Energy:  8.87\n",
      "Energy:  9.11\n",
      "Energy:  7.80\n",
      "Energy:  7.23\n",
      "Energy:  5.64\n",
      "Energy:  4.21\n",
      "Energy:  3.09\n",
      "Energy:  2.07\n",
      "Energy:  2.82\n",
      "Energy:  2.81\n",
      "Energy:  2.80\n",
      "Energy:  2.21\n",
      "Energy:  3.04\n",
      "Energy:  2.23\n",
      "Energy:  3.07\n",
      "Energy:  2.37\n",
      "Energy:  2.61\n",
      "Energy:  3.37\n",
      "Energy:  2.36\n",
      "Energy:  2.79\n",
      "Energy:  2.04\n",
      "Energy:  2.61\n",
      "Energy:  1.91\n",
      "Energy:  2.60\n",
      "Energy:  1.50\n",
      "Energy:  3.06\n",
      "Energy:  1.59\n",
      "-----\n",
      "\n",
      "======TRYING NEGATIVE SAMPLES======\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     for bottom_input, top_input, next_input in dataloader:\n",
    "#         for i in range(ITERATIONS):\n",
    "#             energy = model(bottom_input, top_input)\n",
    "#             # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "#             print(\"Energy:\", f\"{energy.item(): .2f}\")\n",
    "#         print(\"-----\")\n",
    "#     print()\n",
    "\n",
    "# print(\"======TRYING NEGATIVE SAMPLES======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss:  1.59\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m bottom_input, top_input, _ \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ITERATIONS):\n\u001b[0;32m----> 5\u001b[0m         loss \u001b[39m=\u001b[39m model(bottom_input, top_input)\n\u001b[1;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m .2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/non-contrastive-unsupervised/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/projects/non-contrastive-unsupervised/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[138], line 56\u001b[0m, in \u001b[0;36mLayerLocalNetwork.forward\u001b[0;34m(self, bottom_input, top_input)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations[i] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(total_input)\n\u001b[1;32m     55\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_energy()\n\u001b[0;32m---> 56\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m     59\u001b[0m     \u001b[39m# self.optimizers[i]['bottom_up'].zero_grad()\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# self.optimizers[i]['top_down'].zero_grad()\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# self.optimizers[i]['recurrent'].zero_grad()\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers[i][\u001b[39m'\u001b[39m\u001b[39mbottom_up\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/projects/non-contrastive-unsupervised/env/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/non-contrastive-unsupervised/env/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/non-contrastive-unsupervised/env/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"----\")\n",
    "    print()\n",
    "\n",
    "print(\"======TRYING NEGATIVE SAMPLES======\")\n",
    "\n",
    "\n",
    "bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "top_input = torch.eye(10)[1].reshape(1, -1)    # One-hot vector for top input\n",
    "\n",
    "for i in range(75):\n",
    "    loss = model(bottom_input, top_input)\n",
    "    print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "\n",
    "\n",
    "print(\"======TRYING POSITIVE SAMPLE AGAIN======\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"----\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
