{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.05\n",
    "ITERATIONS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class InputPairsDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim):\n",
    "        # Generate pairs of indices, ensuring they match in your desired way\n",
    "        # For simplicity, using identity matrix pairs here as placeholders\n",
    "        self.inputs = [torch.eye(input_dim)[i].reshape(1, -1).squeeze(0) for i in range(input_dim)]\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx % self.input_dim], self.inputs[idx % self.input_dim], self.inputs[(idx + 1) % self.input_dim]\n",
    "    \n",
    "def amplified_initialization_(in_features: int, param: torch.Tensor, amplification_factor: float = 3.0) -> None:\n",
    "    \"\"\"Amplified initialization for Linear layers.\"\"\"\n",
    "    # Compute the standard deviation for He initialization\n",
    "    std = (2.0 / in_features) ** 0.5\n",
    "    # Amplify the standard deviation\n",
    "    amplified_std = std * amplification_factor\n",
    "    # Initialize weights with amplified standard deviation\n",
    "    nn.init.normal_(param, mean=0, std=amplified_std)\n",
    "\n",
    "\n",
    "class LayerLocalNetwork(nn.Module):\n",
    "    def __init__(self, bottom_dim, top_dim, num_layers=1, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            layer = {\n",
    "                'bottom_up': nn.Parameter(torch.randn(bottom_dim, top_dim)),\n",
    "                'top_down': nn.Parameter(torch.randn(top_dim, top_dim)),\n",
    "                'recurrent': nn.Parameter(torch.randn(top_dim, top_dim))\n",
    "            }\n",
    "\n",
    "            if n == num_layers - 1:\n",
    "                amplified_initialization_(bottom_dim, layer['top_down'])\n",
    "            else:\n",
    "                nn.init.uniform_(layer['top_down'], -0.05, 0.05)\n",
    "\n",
    "            nn.init.kaiming_uniform_(layer['bottom_up'])\n",
    "            nn.init.orthogonal_(layer['recurrent'], gain=math.sqrt(2))\n",
    "\n",
    "            self.layers.append(nn.ParameterDict(layer))\n",
    "            self.optimizers.append({\n",
    "                'bottom_up': optim.Adam([layer['bottom_up']], lr=LEARNING_RATE),\n",
    "                'top_down': optim.Adam([layer['top_down']], lr=LEARNING_RATE),\n",
    "                'recurrent': optim.Adam([layer['recurrent']], lr=LEARNING_RATE),\n",
    "            })\n",
    "\n",
    "        self.activations = [torch.zeros(batch_size, top_dim) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, bottom_input, top_input):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].zero_grad()\n",
    "            self.optimizers[i]['top_down'].zero_grad()\n",
    "            self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "        old_activations = [act.detach().clone() for act in self.activations]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            bottom_up_act = torch.mm(bottom_input.detach(), layer['bottom_up']) if i == 0 else torch.mm(\n",
    "                self.activations[i-1], layer['bottom_up'])\n",
    "            top_down_act = torch.mm(top_input.detach(), layer['top_down']) if i == self.num_layers - 1 else torch.mm(\n",
    "                self.activations[i+1], layer['top_down'])\n",
    "            recurrent_act = torch.mm(self.activations[i], layer['recurrent'])\n",
    "\n",
    "            total_input = bottom_up_act + top_down_act + recurrent_act\n",
    "            total_input = F.leaky_relu(total_input)\n",
    "            # print(\"total_input: \", total_input.mean())\n",
    "            self.activations[i] = torch.clamp(total_input, min=-100, max=100)\n",
    "\n",
    "        loss = self.compute_energy(old_activations)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].step()\n",
    "            self.optimizers[i]['top_down'].step()\n",
    "            self.optimizers[i]['recurrent'].step()\n",
    "        \n",
    "        for i in range(0, len(self.activations)):\n",
    "            self.activations[i] = self.activations[i].detach()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_energy(self, old_activations: Tensor):\n",
    "        # Push energy down proportional to activations\n",
    "        running_sum = 0\n",
    "        for act in self.activations:\n",
    "            # pow the activations, average across neurons in layer, average across batches\n",
    "            running_sum += torch.mean(torch.mean(act.pow(2), dim=1), dim=0)\n",
    "        standard_loss = running_sum / len(self.activations)\n",
    "\n",
    "        # Hebbian loss computation: encourage variance at neuron level\n",
    "        hebbian_loss = 0\n",
    "        for act in self.activations:\n",
    "            hebbian_loss += self.generate_lpl_loss_hebbian(act)\n",
    "\n",
    "        # TODO: predictive and decorrelative losses\n",
    "        predictive_loss = 0\n",
    "        for i, act in enumerate(self.activations):\n",
    "            individual_predictive_loss = (act - old_activations[i]) ** 2\n",
    "            individual_predictive_loss = torch.sum(individual_predictive_loss, dim=1)\n",
    "            individual_predictive_loss = torch.sum(individual_predictive_loss, dim=0)\n",
    "            individual_predictive_loss = individual_predictive_loss / (2 * act.shape[0] * act.shape[1])\n",
    "            predictive_loss += individual_predictive_loss\n",
    "\n",
    "        # Combine losses\n",
    "        standard_loss_scale = 30\n",
    "        hebbian_loss_scale = 1\n",
    "        predictive_loss_scale = 1\n",
    "        # print(f\"s: {standard_loss_scale * standard_loss} | h: {hebbian_loss} | p: {predictive_loss}\")\n",
    "        total_loss = standard_loss_scale * standard_loss +  hebbian_loss_scale * hebbian_loss + predictive_loss_scale * predictive_loss  # Consider weighting factors if necessary\n",
    "        # total_loss = standard_loss\n",
    "        # print(f\"standard_loss: {standard_loss} | hebbian_loss: {hebbian_loss}\")\n",
    "        return total_loss\n",
    "\n",
    "    def generate_lpl_loss_hebbian(self, activations):\n",
    "        mean_act = torch.mean(activations, dim=0)\n",
    "        mean_subtracted = activations - mean_act\n",
    "        sigma_squared = torch.sum(mean_subtracted ** 2, dim=0) / (activations.shape[0] - 1)\n",
    "        loss = -torch.log(sigma_squared + 1e-10).sum() / sigma_squared.shape[0]\n",
    "        return loss\n",
    "\n",
    "dataset = InputPairsDataset(num_samples=100, input_dim=INPUT_DIM)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Example usage:\n",
    "model = LayerLocalNetwork(bottom_dim=INPUT_DIM, top_dim=INPUT_DIM, num_layers=NUM_LAYERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     for bottom_input, top_input, next_input in dataloader:\n",
    "#         for i in range(ITERATIONS):\n",
    "#             energy = model(bottom_input, top_input)\n",
    "#             # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "#             print(\"Energy:\", f\"{energy.item(): .2f}\")\n",
    "#         print(\"-----\")\n",
    "#     print()\n",
    "\n",
    "# print(\"======TRYING NEGATIVE SAMPLES======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\n",
      "Epoch: 1\n",
      "Average Loss:  13.490\n",
      "Average Loss:  12.743\n",
      "Average Loss:  11.084\n",
      "Average Loss:  11.084\n",
      "Average Loss:  12.042\n",
      "Average Loss:  14.323\n",
      "Average Loss:  14.142\n",
      "Average Loss:  14.545\n",
      "Average Loss:  12.933\n",
      "Average Loss:  13.191\n",
      "Average Loss:  12.236\n",
      "Average Loss:  12.544\n",
      "Average Loss:  11.820\n",
      "Average Loss:  13.345\n",
      "Average Loss:  12.886\n",
      "Average Loss:  11.850\n",
      "Average Loss:  11.195\n",
      "Average Loss:  12.451\n",
      "Average Loss:  14.167\n",
      "Average Loss:  14.546\n",
      "\n",
      "Epoch: 2\n",
      "Average Loss:  12.503\n",
      "Average Loss:  13.723\n",
      "Average Loss:  13.089\n",
      "Average Loss:  15.354\n",
      "Average Loss:  14.838\n",
      "Average Loss:  12.897\n",
      "Average Loss:  13.807\n",
      "Average Loss:  13.315\n",
      "Average Loss:  12.709\n",
      "Average Loss:  11.708\n",
      "Average Loss:  11.469\n",
      "Average Loss:  11.614\n",
      "Average Loss:  12.865\n",
      "Average Loss:  16.015\n",
      "Average Loss:  14.363\n",
      "Average Loss:  11.885\n",
      "Average Loss:  15.578\n",
      "Average Loss:  16.556\n",
      "Average Loss:  15.036\n",
      "Average Loss:  13.682\n",
      "\n",
      "Epoch: 3\n",
      "Average Loss:  13.823\n",
      "Average Loss:  14.175\n",
      "Average Loss:  12.688\n",
      "Average Loss:  14.407\n",
      "Average Loss:  13.523\n",
      "Average Loss:  12.796\n",
      "Average Loss:  12.872\n",
      "Average Loss:  13.008\n",
      "Average Loss:  16.700\n",
      "Average Loss:  15.383\n",
      "Average Loss:  15.707\n",
      "Average Loss:  16.319\n",
      "Average Loss:  16.734\n",
      "Average Loss:  16.106\n",
      "Average Loss:  13.979\n",
      "Average Loss:  14.243\n",
      "Average Loss:  12.929\n",
      "Average Loss:  13.600\n",
      "Average Loss:  13.196\n",
      "Average Loss:  17.794\n",
      "\n",
      "Epoch: 4\n",
      "Average Loss:  14.687\n",
      "Average Loss:  12.836\n",
      "Average Loss:  14.042\n",
      "Average Loss:  24.835\n",
      "Average Loss:  15.642\n",
      "Average Loss:  17.447\n",
      "Average Loss:  13.730\n",
      "Average Loss:  13.712\n",
      "Average Loss:  16.595\n",
      "Average Loss:  13.670\n",
      "Average Loss:  13.502\n",
      "Average Loss:  13.379\n",
      "Average Loss:  13.009\n",
      "Average Loss:  12.378\n",
      "Average Loss:  13.476\n",
      "Average Loss:  12.859\n",
      "Average Loss:  14.257\n",
      "Average Loss:  15.364\n",
      "Average Loss:  14.497\n",
      "Average Loss:  16.572\n",
      "\n",
      "Epoch: 5\n",
      "Average Loss:  12.526\n",
      "Average Loss:  15.633\n",
      "Average Loss:  13.922\n",
      "Average Loss:  14.123\n",
      "Average Loss:  17.547\n",
      "Average Loss:  14.920\n",
      "Average Loss:  17.510\n",
      "Average Loss:  15.579\n",
      "Average Loss:  13.979\n",
      "Average Loss:  12.900\n",
      "Average Loss:  14.731\n",
      "Average Loss:  15.397\n",
      "Average Loss:  13.304\n",
      "Average Loss:  13.915\n",
      "Average Loss:  15.422\n",
      "Average Loss:  13.957\n",
      "Average Loss:  15.211\n",
      "Average Loss:  15.069\n",
      "Average Loss:  14.736\n",
      "Average Loss:  15.033\n",
      "\n",
      "Epoch: 6\n",
      "Average Loss:  12.529\n",
      "Average Loss:  12.667\n",
      "Average Loss:  14.262\n",
      "Average Loss:  14.561\n",
      "Average Loss:  12.849\n",
      "Average Loss:  13.239\n",
      "Average Loss:  13.820\n",
      "Average Loss:  12.655\n",
      "Average Loss:  13.503\n",
      "Average Loss:  13.104\n",
      "Average Loss:  14.083\n",
      "Average Loss:  13.281\n",
      "Average Loss:  15939.527\n",
      "Average Loss:  13030.619\n",
      "Average Loss:  6207.547\n",
      "Average Loss:  5718.126\n",
      "Average Loss:  9507.831\n",
      "Average Loss:  3943.295\n",
      "Average Loss:  19.809\n",
      "Average Loss:  67.189\n",
      "\n",
      "Epoch: 7\n",
      "Average Loss:  16.132\n",
      "Average Loss:  190.569\n",
      "Average Loss:  24.091\n",
      "Average Loss:  29.678\n",
      "Average Loss:  28.140\n",
      "Average Loss:  21.102\n",
      "Average Loss:  21.202\n",
      "Average Loss:  17.145\n",
      "Average Loss:  32.166\n",
      "Average Loss:  17.452\n",
      "Average Loss:  24.964\n",
      "Average Loss:  16.583\n",
      "Average Loss:  25.140\n",
      "Average Loss:  19.987\n",
      "Average Loss:  17.182\n",
      "Average Loss:  19.234\n",
      "Average Loss:  16.573\n",
      "Average Loss:  18.422\n",
      "Average Loss:  17.651\n",
      "Average Loss:  19.008\n",
      "\n",
      "Epoch: 8\n",
      "Average Loss:  47.291\n",
      "Average Loss:  18.135\n",
      "Average Loss:  20.514\n",
      "Average Loss:  16.995\n",
      "Average Loss:  16.388\n",
      "Average Loss:  19.750\n",
      "Average Loss:  20.565\n",
      "Average Loss:  17.896\n",
      "Average Loss:  253.947\n",
      "Average Loss:  16.764\n",
      "Average Loss:  17.591\n",
      "Average Loss:  19.065\n",
      "Average Loss:  26.036\n",
      "Average Loss:  18.281\n",
      "Average Loss:  24.105\n",
      "Average Loss:  24.160\n",
      "Average Loss:  22.853\n",
      "Average Loss:  16.640\n",
      "Average Loss:  24.598\n",
      "Average Loss:  18.260\n",
      "\n",
      "Epoch: 9\n",
      "Average Loss:  17.929\n",
      "Average Loss:  16.979\n",
      "Average Loss:  15.525\n",
      "Average Loss:  17.793\n",
      "Average Loss:  17.487\n",
      "Average Loss:  17.167\n",
      "Average Loss:  17.183\n",
      "Average Loss:  16.497\n",
      "Average Loss:  66.996\n",
      "Average Loss:  15.901\n",
      "Average Loss:  15.577\n",
      "Average Loss:  15.966\n",
      "Average Loss:  16.526\n",
      "Average Loss:  16.759\n",
      "Average Loss:  17.541\n",
      "Average Loss:  14.963\n",
      "Average Loss:  15.024\n",
      "Average Loss:  14.732\n",
      "Average Loss:  18.095\n",
      "Average Loss:  14.910\n",
      "\n",
      "Epoch: 10\n",
      "Average Loss:  19.311\n",
      "Average Loss:  15.888\n",
      "Average Loss:  15.239\n",
      "Average Loss:  15.391\n",
      "Average Loss:  26.820\n",
      "Average Loss:  15.463\n",
      "Average Loss:  18.306\n",
      "Average Loss:  17.900\n",
      "Average Loss:  17.846\n",
      "Average Loss:  14.410\n",
      "Average Loss:  16.189\n",
      "Average Loss:  16.045\n",
      "Average Loss:  18.035\n",
      "Average Loss:  14.541\n",
      "Average Loss:  40.499\n",
      "Average Loss:  16.565\n",
      "Average Loss:  15.012\n",
      "Average Loss:  15.171\n",
      "Average Loss:  15.892\n",
      "Average Loss:  17.610\n",
      "\n",
      "Epoch: 11\n",
      "Average Loss:  17.455\n",
      "Average Loss:  19.004\n",
      "Average Loss:  14.766\n",
      "Average Loss:  14.577\n",
      "Average Loss:  15.279\n",
      "Average Loss:  14.859\n",
      "Average Loss:  14.686\n",
      "Average Loss:  15.202\n",
      "Average Loss:  16.310\n",
      "Average Loss:  16.312\n",
      "Average Loss:  16.207\n",
      "Average Loss:  19.561\n",
      "Average Loss:  15.940\n",
      "Average Loss:  17.543\n",
      "Average Loss:  16.712\n",
      "Average Loss:  15.725\n",
      "Average Loss:  14.928\n",
      "Average Loss:  14.587\n",
      "Average Loss:  15.981\n",
      "Average Loss:  442.988\n",
      "\n",
      "Epoch: 12\n",
      "Average Loss:  17.642\n",
      "Average Loss:  16.538\n",
      "Average Loss:  18.694\n",
      "Average Loss:  17.383\n",
      "Average Loss:  16.719\n",
      "Average Loss:  15.265\n",
      "Average Loss:  18.857\n",
      "Average Loss:  16.636\n",
      "Average Loss:  23.484\n",
      "Average Loss:  18.632\n",
      "Average Loss:  22.536\n",
      "Average Loss:  15.908\n",
      "Average Loss:  18.311\n",
      "Average Loss:  17.408\n",
      "Average Loss:  16.508\n",
      "Average Loss:  15.251\n",
      "Average Loss:  15.900\n",
      "Average Loss:  15.622\n",
      "Average Loss:  31.264\n",
      "Average Loss:  16.343\n",
      "\n",
      "Epoch: 13\n",
      "Average Loss:  15.665\n",
      "Average Loss:  16.749\n",
      "Average Loss:  16.770\n",
      "Average Loss:  33614.721\n",
      "Average Loss:  68827.948\n",
      "Average Loss:  102716.028\n",
      "Average Loss:  132986.266\n",
      "Average Loss:  136154.327\n",
      "Average Loss:  136056.114\n",
      "Average Loss:  136021.681\n",
      "Average Loss:  136003.426\n",
      "Average Loss:  135997.424\n",
      "Average Loss:  135995.059\n",
      "Average Loss:  135998.497\n",
      "Average Loss:  162285.994\n",
      "Average Loss:  170697.085\n",
      "Average Loss:  170414.469\n",
      "Average Loss:  170294.638\n",
      "Average Loss:  170192.951\n",
      "Average Loss:  170307.897\n",
      "\n",
      "Epoch: 14\n",
      "Average Loss:  170909.963\n",
      "Average Loss:  170433.849\n",
      "Average Loss:  170269.721\n",
      "Average Loss:  170158.203\n",
      "Average Loss:  170149.616\n",
      "Average Loss:  170072.631\n",
      "Average Loss:  170351.422\n",
      "Average Loss:  170745.801\n",
      "Average Loss:  170349.694\n",
      "Average Loss:  170372.330\n",
      "Average Loss:  170152.293\n",
      "Average Loss:  170701.358\n",
      "Average Loss:  171055.592\n",
      "Average Loss:  170585.196\n",
      "Average Loss:  170326.923\n",
      "Average Loss:  170205.072\n",
      "Average Loss:  170140.421\n",
      "Average Loss:  170101.722\n",
      "Average Loss:  170169.759\n",
      "Average Loss:  170121.299\n",
      "\n",
      "Epoch: 15\n",
      "Average Loss:  170079.796\n",
      "Average Loss:  170060.392\n",
      "Average Loss:  170053.010\n",
      "Average Loss:  170049.867\n",
      "Average Loss:  170110.183\n",
      "Average Loss:  170127.103\n",
      "Average Loss:  170069.251\n",
      "Average Loss:  170052.760\n",
      "Average Loss:  170054.362\n",
      "Average Loss:  170096.697\n",
      "Average Loss:  170063.775\n",
      "Average Loss:  170053.972\n",
      "Average Loss:  170105.450\n",
      "Average Loss:  170079.975\n",
      "Average Loss:  170097.135\n",
      "Average Loss:  170114.321\n",
      "Average Loss:  170069.503\n",
      "Average Loss:  170137.552\n",
      "Average Loss:  170091.882\n",
      "Average Loss:  170056.258\n",
      "\n",
      "Epoch: 16\n",
      "Average Loss:  170205.263\n",
      "Average Loss:  170609.174\n",
      "Average Loss:  170300.565\n",
      "Average Loss:  170148.643\n",
      "Average Loss:  170099.390\n",
      "Average Loss:  170080.946\n",
      "Average Loss:  170248.732\n",
      "Average Loss:  170113.022\n",
      "Average Loss:  170082.370\n",
      "Average Loss:  170055.250\n",
      "Average Loss:  170142.533\n",
      "Average Loss:  170850.884\n",
      "Average Loss:  170556.414\n",
      "Average Loss:  170319.715\n",
      "Average Loss:  170198.198\n",
      "Average Loss:  170145.632\n",
      "Average Loss:  170095.714\n",
      "Average Loss:  170083.465\n",
      "Average Loss:  170067.983\n",
      "Average Loss:  170071.506\n",
      "\n",
      "Epoch: 17\n",
      "Average Loss:  170059.333\n",
      "Average Loss:  170054.315\n",
      "Average Loss:  170062.363\n",
      "Average Loss:  170058.588\n",
      "Average Loss:  170424.194\n",
      "Average Loss:  170637.378\n",
      "Average Loss:  170297.983\n",
      "Average Loss:  170163.457\n",
      "Average Loss:  170112.213\n",
      "Average Loss:  167111.517\n",
      "Average Loss:  160929.203\n",
      "Average Loss:  160540.857\n",
      "Average Loss:  160361.352\n",
      "Average Loss:  160269.726\n",
      "Average Loss:  160215.236\n",
      "Average Loss:  160184.908\n",
      "Average Loss:  160161.344\n",
      "Average Loss:  160125.133\n",
      "Average Loss:  160114.217\n",
      "Average Loss:  160109.423\n",
      "\n",
      "Epoch: 18\n",
      "Average Loss:  160082.003\n",
      "Average Loss:  160074.101\n",
      "Average Loss:  160090.713\n",
      "Average Loss:  160072.689\n",
      "Average Loss:  160064.340\n",
      "Average Loss:  160076.110\n",
      "Average Loss:  160058.591\n",
      "Average Loss:  160077.367\n",
      "Average Loss:  160204.481\n",
      "Average Loss:  160081.749\n",
      "Average Loss:  160606.653\n",
      "Average Loss:  160304.983\n",
      "Average Loss:  160136.404\n",
      "Average Loss:  160085.426\n",
      "Average Loss:  160065.949\n",
      "Average Loss:  160057.483\n",
      "Average Loss:  160055.231\n",
      "Average Loss:  160267.885\n",
      "Average Loss:  160495.418\n",
      "Average Loss:  160233.966\n",
      "\n",
      "Epoch: 19\n",
      "Average Loss:  160130.510\n",
      "Average Loss:  160086.836\n",
      "Average Loss:  160065.320\n",
      "Average Loss:  160056.876\n",
      "Average Loss:  160056.127\n",
      "Average Loss:  160105.625\n",
      "Average Loss:  160103.781\n",
      "Average Loss:  160079.959\n",
      "Average Loss:  160104.304\n",
      "Average Loss:  160053.548\n",
      "Average Loss:  160049.730\n",
      "Average Loss:  160089.028\n",
      "Average Loss:  160138.538\n",
      "Average Loss:  160084.950\n",
      "Average Loss:  160138.351\n",
      "Average Loss:  160061.388\n",
      "Average Loss:  160123.546\n",
      "Average Loss:  160112.865\n",
      "Average Loss:  160239.366\n",
      "Average Loss:  160105.659\n",
      "\n",
      "======TRYING POSITIVE SAMPLE AGAIN======\n",
      "Epoch: 0\n",
      "Average Loss:  160084.838\n",
      "Average Loss:  160074.685\n",
      "Average Loss:  160072.155\n",
      "Average Loss:  160070.015\n",
      "Average Loss:  160069.364\n",
      "Average Loss:  168858.069\n",
      "Average Loss:  167053.612\n",
      "Average Loss:  166618.848\n",
      "Average Loss:  166845.117\n",
      "Average Loss:  166463.946\n",
      "Average Loss:  166212.524\n",
      "Average Loss:  172573.008\n",
      "Average Loss:  178043.267\n",
      "Average Loss:  166985.942\n",
      "Average Loss:  166766.247\n",
      "Average Loss:  166655.083\n",
      "Average Loss:  166443.879\n",
      "Average Loss:  166275.952\n",
      "Average Loss:  166143.527\n",
      "Average Loss:  166051.806\n",
      "\n",
      "Epoch: 1\n",
      "Average Loss:  165965.923\n",
      "Average Loss:  165902.705\n",
      "Average Loss:  196620.604\n",
      "Average Loss:  189680.641\n",
      "Average Loss:  192351.735\n",
      "Average Loss:  195724.506\n",
      "Average Loss:  197705.328\n",
      "Average Loss:  197341.102\n",
      "Average Loss:  197129.970\n",
      "Average Loss:  197027.349\n",
      "Average Loss:  196897.490\n",
      "Average Loss:  196973.602\n",
      "Average Loss:  197114.807\n",
      "Average Loss:  197080.671\n",
      "Average Loss:  196941.187\n",
      "Average Loss:  195014.282\n",
      "Average Loss:  192734.237\n",
      "Average Loss:  192315.807\n",
      "Average Loss:  191991.546\n",
      "Average Loss:  191780.641\n",
      "\n",
      "Epoch: 2\n",
      "Average Loss:  191616.670\n",
      "Average Loss:  191482.533\n",
      "Average Loss:  191386.373\n",
      "Average Loss:  191321.765\n",
      "Average Loss:  191374.437\n",
      "Average Loss:  191315.571\n",
      "Average Loss:  191257.605\n",
      "Average Loss:  191509.473\n",
      "Average Loss:  191478.731\n",
      "Average Loss:  191376.043\n",
      "Average Loss:  191302.283\n",
      "Average Loss:  191271.585\n",
      "Average Loss:  191220.171\n",
      "Average Loss:  191214.548\n",
      "Average Loss:  191196.150\n",
      "Average Loss:  191155.011\n",
      "Average Loss:  192348.612\n",
      "Average Loss:  192175.651\n",
      "Average Loss:  192156.197\n",
      "Average Loss:  192048.884\n",
      "\n",
      "Epoch: 3\n",
      "Average Loss:  191864.648\n",
      "Average Loss:  191868.810\n",
      "Average Loss:  191712.849\n",
      "Average Loss:  191617.098\n",
      "Average Loss:  191465.830\n",
      "Average Loss:  191383.652\n",
      "Average Loss:  191310.647\n",
      "Average Loss:  191282.064\n",
      "Average Loss:  191587.057\n",
      "Average Loss:  192055.135\n",
      "Average Loss:  191924.677\n",
      "Average Loss:  191779.649\n",
      "Average Loss:  191666.342\n",
      "Average Loss:  191562.918\n",
      "Average Loss:  191489.851\n",
      "Average Loss:  194289.705\n",
      "Average Loss:  192132.602\n",
      "Average Loss:  192116.943\n",
      "Average Loss:  191912.491\n",
      "Average Loss:  191731.578\n",
      "\n",
      "Epoch: 4\n",
      "Average Loss:  191693.196\n",
      "Average Loss:  191518.518\n",
      "Average Loss:  191415.898\n",
      "Average Loss:  191362.896\n",
      "Average Loss:  191493.314\n",
      "Average Loss:  191393.077\n",
      "Average Loss:  191451.576\n",
      "Average Loss:  191451.705\n",
      "Average Loss:  191366.389\n",
      "Average Loss:  191694.505\n",
      "Average Loss:  191849.780\n",
      "Average Loss:  191770.422\n",
      "Average Loss:  191571.088\n",
      "Average Loss:  191454.355\n",
      "Average Loss:  191355.463\n",
      "Average Loss:  191277.419\n",
      "Average Loss:  191220.748\n",
      "Average Loss:  191190.320\n",
      "Average Loss:  191225.339\n",
      "Average Loss:  191192.638\n",
      "\n",
      "======TRYING NEGATIVE SAMPLES======\n",
      "Loss:  181146.39\n",
      "Loss:  201188.55\n",
      "Loss:  181145.23\n",
      "Loss:  201186.98\n",
      "Loss:  181143.73\n",
      "Loss:  201185.36\n",
      "Loss:  181142.20\n",
      "Loss:  201183.77\n",
      "Loss:  181140.69\n",
      "Loss:  201182.20\n",
      "Loss:  181139.17\n",
      "Loss:  201180.64\n",
      "Loss:  181137.69\n",
      "Loss:  201179.12\n",
      "Loss:  181136.23\n",
      "Loss:  201177.58\n",
      "Loss:  181134.83\n",
      "Loss:  201176.06\n",
      "Loss:  181133.39\n",
      "Loss:  201174.59\n",
      "Loss:  181132.55\n",
      "Loss:  201174.64\n",
      "Loss:  181130.53\n",
      "Loss:  201172.34\n",
      "Loss:  181129.08\n",
      "Loss:  201171.06\n",
      "Loss:  181127.69\n",
      "Loss:  201169.84\n",
      "Loss:  181126.33\n",
      "Loss:  201168.55\n",
      "Average Loss:  191157.234\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        layer_activations_queue = deque(maxlen=10)\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "\n",
    "            # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "            # layer_activations_queue.append(layer_activations)\n",
    "\n",
    "            # input_to_decoder = torch.stack(list(layer_activations_queue), dim=1)\n",
    "            # print(\"layer activations shape: \", layer_activations.shape)\n",
    "            # print(\"shape: \", input_to_decoder.shape)\n",
    "        # if epoch > num_epochs - 3:\n",
    "        if epoch > 0:\n",
    "            print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n",
    "\n",
    "# print(\"======TRYING NEGATIVE SAMPLES======\")\n",
    "\n",
    "# for optimizer_dict in model.optimizers:\n",
    "#     for optimizer in optimizer_dict.values():\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             # param_group['lr'] = param_group['lr'] / BATCH_SIZE\n",
    "#             param_group['lr'] = 0\n",
    "\n",
    "\n",
    "# bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "# top_input = torch.eye(10)[1].reshape(1, -1)    # One-hot vector for top input\n",
    "# assert bottom_input.shape == (1, INPUT_DIM)\n",
    "# assert top_input.shape == (1, INPUT_DIM)\n",
    "\n",
    "# running_sum = 0\n",
    "# negative_iterations = 30\n",
    "# for i in range(negative_iterations):\n",
    "#     loss = model(bottom_input, top_input)\n",
    "#     running_sum += loss.item()\n",
    "#     print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "# print(\"Average Loss:\", f\"{running_sum / negative_iterations: .3f}\")\n",
    "\n",
    "\n",
    "print(\"======TRYING POSITIVE SAMPLE AGAIN======\")\n",
    "\n",
    "# bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "# top_input = torch.eye(10)[0].reshape(1, -1)    # One-hot vector for top input\n",
    "\n",
    "# running_sum = 0\n",
    "# positive_iterations = 30\n",
    "# for i in range(positive_iterations):\n",
    "#     loss = model(bottom_input, top_input)\n",
    "#     running_sum += loss.item()\n",
    "#     print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "# print(\"Average Loss:\", f\"{running_sum / positive_iterations: .3f}\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            # bottom_input = bottom_input[0].unsqueeze(0).repeat(BATCH_SIZE, 1)\n",
    "            # top_input = bottom_input[0].unsqueeze(0).repeat(BATCH_SIZE, 1)\n",
    "            # assert bottom_input.shape == (BATCH_SIZE, INPUT_DIM)\n",
    "            # assert top_input.shape == (BATCH_SIZE, INPUT_DIM)\n",
    "            bottom_input = bottom_input[0].unsqueeze(0)\n",
    "            top_input = bottom_input[0].unsqueeze(0)\n",
    "            assert bottom_input.shape == (1, INPUT_DIM)\n",
    "            assert top_input.shape == (1, INPUT_DIM)\n",
    "\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"======TRYING NEGATIVE SAMPLES======\")\n",
    "\n",
    "bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "top_input = torch.eye(10)[1].reshape(1, -1)    # One-hot vector for top input\n",
    "assert bottom_input.shape == (1, INPUT_DIM)\n",
    "assert top_input.shape == (1, INPUT_DIM)\n",
    "\n",
    "running_sum = 0\n",
    "negative_iterations = 30\n",
    "for i in range(negative_iterations):\n",
    "    loss = model(bottom_input, top_input)\n",
    "    running_sum += loss.item()\n",
    "    print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "print(\"Average Loss:\", f\"{running_sum / negative_iterations: .3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
