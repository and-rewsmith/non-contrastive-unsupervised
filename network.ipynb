{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.05\n",
    "ITERATIONS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputPairsDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim):\n",
    "        # Generate pairs of indices, ensuring they match in your desired way\n",
    "        # For simplicity, using identity matrix pairs here as placeholders\n",
    "        self.inputs = [torch.eye(input_dim)[i].reshape(1, -1).squeeze(0) for i in range(input_dim)]\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx % self.input_dim], self.inputs[idx % self.input_dim], self.inputs[(idx + 1) % self.input_dim]\n",
    "\n",
    "\n",
    "class LayerLocalNetwork(nn.Module):\n",
    "    def __init__(self, bottom_dim, top_dim, num_layers=1, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layer = {\n",
    "                'bottom_up': nn.Parameter(torch.randn(bottom_dim, top_dim)),\n",
    "                'top_down': nn.Parameter(torch.randn(top_dim, top_dim)),\n",
    "                'recurrent': nn.Parameter(torch.randn(top_dim, top_dim))\n",
    "            }\n",
    "            self.layers.append(nn.ParameterDict(layer))\n",
    "            self.optimizers.append({\n",
    "                'bottom_up': optim.Adam([layer['bottom_up']], lr=LEARNING_RATE),\n",
    "                'top_down': optim.Adam([layer['top_down']], lr=LEARNING_RATE),\n",
    "                'recurrent': optim.Adam([layer['recurrent']], lr=LEARNING_RATE),\n",
    "            })\n",
    "\n",
    "        self.activations = [torch.zeros(batch_size, top_dim) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, bottom_input, top_input):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].zero_grad()\n",
    "            self.optimizers[i]['top_down'].zero_grad()\n",
    "            self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            bottom_up_act = torch.mm(bottom_input.detach(), layer['bottom_up']) if i == 0 else torch.mm(\n",
    "                self.activations[i-1], layer['bottom_up'])\n",
    "            top_down_act = torch.mm(top_input.detach(), layer['top_down']) if i == self.num_layers - 1 else torch.mm(\n",
    "                self.activations[i+1], layer['top_down'])\n",
    "            recurrent_act = torch.mm(self.activations[i], layer['recurrent'])\n",
    "\n",
    "            total_input = bottom_up_act + top_down_act + recurrent_act\n",
    "            self.activations[i] = F.leaky_relu(total_input)\n",
    "\n",
    "        loss = self.compute_energy()\n",
    "        loss.backward()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].step()\n",
    "            self.optimizers[i]['top_down'].step()\n",
    "            self.optimizers[i]['recurrent'].step()\n",
    "        \n",
    "        for i in range(0, len(self.activations)):\n",
    "            self.activations[i] = self.activations[i].detach()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_energy(self):\n",
    "        # Push energy down proportional to activations\n",
    "        running_sum = 0\n",
    "        for act in self.activations:\n",
    "            running_sum += torch.mean(act.pow(2))\n",
    "        standard_loss = running_sum / len(self.activations)\n",
    "\n",
    "        # Hebbian loss computation: encourage variance at neuron level\n",
    "        hebbian_loss = 0\n",
    "        for act in self.activations:\n",
    "            hebbian_loss += self.generate_lpl_loss_hebbian(act)\n",
    "\n",
    "        # TODO: predictive and decorrelative losses\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = standard_loss + hebbian_loss  # Consider weighting factors if necessary\n",
    "        # total_loss = standard_loss\n",
    "        return total_loss\n",
    "\n",
    "    def generate_lpl_loss_hebbian(self, activations):\n",
    "        mean_act = torch.mean(activations, dim=0)\n",
    "        mean_subtracted = activations - mean_act\n",
    "        sigma_squared = torch.sum(mean_subtracted ** 2, dim=0) / (activations.shape[0] - 1)\n",
    "        loss = -torch.log(sigma_squared + 1e-10).sum() / sigma_squared.shape[0]\n",
    "        return loss\n",
    "\n",
    "dataset = InputPairsDataset(num_samples=100, input_dim=INPUT_DIM)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Example usage:\n",
    "model = LayerLocalNetwork(bottom_dim=INPUT_DIM, top_dim=INPUT_DIM, num_layers=NUM_LAYERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     for bottom_input, top_input, next_input in dataloader:\n",
    "#         for i in range(ITERATIONS):\n",
    "#             energy = model(bottom_input, top_input)\n",
    "#             # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "#             print(\"Energy:\", f\"{energy.item(): .2f}\")\n",
    "#         print(\"-----\")\n",
    "#     print()\n",
    "\n",
    "# print(\"======TRYING NEGATIVE SAMPLES======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Average Loss:  489100686799.786\n",
      "Average Loss:  40696.814\n",
      "Average Loss:  3.216\n",
      "Average Loss:  10.169\n",
      "Average Loss:  6.319\n",
      "Average Loss:  3.936\n",
      "Average Loss:  3.408\n",
      "Average Loss:  3.608\n",
      "Average Loss:  3.839\n",
      "Average Loss:  2.311\n",
      "Average Loss:  2.649\n",
      "Average Loss:  3.981\n",
      "Average Loss:  3.046\n",
      "Average Loss:  3.346\n",
      "Average Loss:  2.412\n",
      "Average Loss:  4.845\n",
      "Average Loss:  4.373\n",
      "Average Loss:  3.387\n",
      "Average Loss:  2.393\n",
      "Average Loss:  2.740\n",
      "\n",
      "======TRYING NEGATIVE SAMPLES======\n",
      "Loss:  6.66\n",
      "Loss:  7.23\n",
      "Loss:  6.78\n",
      "Loss:  6.86\n",
      "Loss:  7.03\n",
      "Loss:  7.15\n",
      "Loss:  7.16\n",
      "Loss:  7.28\n",
      "Loss:  7.33\n",
      "Loss:  7.45\n",
      "Loss:  7.53\n",
      "Loss:  7.69\n",
      "Loss:  7.78\n",
      "Loss:  7.94\n",
      "Loss:  8.00\n",
      "Loss:  8.37\n",
      "Loss:  8.98\n",
      "Loss:  8.77\n",
      "Loss:  8.76\n",
      "Loss:  8.50\n",
      "Loss:  8.39\n",
      "Loss:  8.45\n",
      "Loss:  8.47\n",
      "Loss:  8.49\n",
      "Loss:  8.43\n",
      "Loss:  8.35\n",
      "Loss:  8.24\n",
      "Loss:  8.17\n",
      "Loss:  8.09\n",
      "Loss:  8.00\n",
      "Loss:  7.91\n",
      "Loss:  7.83\n",
      "Loss:  7.76\n",
      "Loss:  7.71\n",
      "Loss:  7.68\n",
      "Loss:  7.72\n",
      "Loss:  7.74\n",
      "Loss:  7.73\n",
      "Loss:  7.69\n",
      "Loss:  7.66\n",
      "Loss:  7.64\n",
      "Loss:  7.68\n",
      "Loss:  7.71\n",
      "Loss:  7.77\n",
      "Loss:  7.81\n",
      "Loss:  7.89\n",
      "Loss:  7.93\n",
      "Loss:  7.99\n",
      "Loss:  8.00\n",
      "Loss:  8.04\n",
      "Loss:  8.04\n",
      "Loss:  8.07\n",
      "Loss:  8.09\n",
      "Loss:  8.13\n",
      "Loss:  8.13\n",
      "Loss:  8.12\n",
      "Loss:  8.08\n",
      "Loss:  8.07\n",
      "Loss:  8.06\n",
      "Loss:  8.08\n",
      "Loss:  8.03\n",
      "Loss:  7.99\n",
      "Loss:  7.96\n",
      "Loss:  7.97\n",
      "Loss:  7.93\n",
      "Loss:  7.90\n",
      "Loss:  7.88\n",
      "Loss:  7.89\n",
      "Loss:  7.88\n",
      "Loss:  7.86\n",
      "Loss:  7.84\n",
      "Loss:  7.87\n",
      "Loss:  7.85\n",
      "Loss:  7.87\n",
      "Loss:  7.87\n",
      "Average Loss:  7.889\n",
      "======TRYING POSITIVE SAMPLE AGAIN======\n",
      "Epoch: 0\n",
      "Average Loss:  2.834\n",
      "Average Loss:  3.582\n",
      "Average Loss:  2.873\n",
      "Average Loss:  3.284\n",
      "Average Loss:  5.469\n",
      "Average Loss:  3.167\n",
      "Average Loss:  2.681\n",
      "Average Loss:  2.123\n",
      "Average Loss:  2.273\n",
      "Average Loss:  2.208\n",
      "Average Loss:  2.546\n",
      "Average Loss:  2.827\n",
      "Average Loss:  3.334\n",
      "Average Loss:  2.418\n",
      "Average Loss:  3.493\n",
      "Average Loss:  3.312\n",
      "Average Loss:  3.180\n",
      "Average Loss:  2.462\n",
      "Average Loss:  2.386\n",
      "Average Loss:  3.696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        layer_activations_queue = deque(maxlen=10)\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "\n",
    "            # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "            # layer_activations_queue.append(layer_activations)\n",
    "\n",
    "            # input_to_decoder = torch.stack(list(layer_activations_queue), dim=1)\n",
    "            # print(\"layer activations shape: \", layer_activations.shape)\n",
    "            # print(\"shape: \", input_to_decoder.shape)\n",
    "\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n",
    "\n",
    "print(\"======TRYING NEGATIVE SAMPLES======\")\n",
    "\n",
    "\n",
    "bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "top_input = torch.eye(10)[1].reshape(1, -1)    # One-hot vector for top input\n",
    "\n",
    "running_sum = 0\n",
    "for i in range(75):\n",
    "    loss = model(bottom_input, top_input)\n",
    "    running_sum += loss.item()\n",
    "    print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "print(\"Average Loss:\", f\"{running_sum / 75: .3f}\")\n",
    "\n",
    "\n",
    "print(\"======TRYING POSITIVE SAMPLE AGAIN======\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
