{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.05\n",
    "ITERATIONS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputPairsDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim):\n",
    "        # Generate pairs of indices, ensuring they match in your desired way\n",
    "        # For simplicity, using identity matrix pairs here as placeholders\n",
    "        self.inputs = [torch.eye(input_dim)[i].reshape(1, -1).squeeze(0) for i in range(input_dim)]\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx % self.input_dim], self.inputs[idx % self.input_dim], self.inputs[(idx + 1) % self.input_dim]\n",
    "\n",
    "\n",
    "class LayerLocalNetwork(nn.Module):\n",
    "    def __init__(self, bottom_dim, top_dim, num_layers=1, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layer = {\n",
    "                'bottom_up': nn.Parameter(torch.randn(bottom_dim, top_dim)),\n",
    "                'top_down': nn.Parameter(torch.randn(top_dim, top_dim)),\n",
    "                'recurrent': nn.Parameter(torch.randn(top_dim, top_dim))\n",
    "            }\n",
    "            self.layers.append(nn.ParameterDict(layer))\n",
    "            self.optimizers.append({\n",
    "                'bottom_up': optim.Adam([layer['bottom_up']], lr=LEARNING_RATE),\n",
    "                'top_down': optim.Adam([layer['top_down']], lr=LEARNING_RATE),\n",
    "                'recurrent': optim.Adam([layer['recurrent']], lr=LEARNING_RATE),\n",
    "            })\n",
    "\n",
    "        self.activations = [torch.zeros(batch_size, top_dim) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, bottom_input, top_input):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].zero_grad()\n",
    "            self.optimizers[i]['top_down'].zero_grad()\n",
    "            self.optimizers[i]['recurrent'].zero_grad()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            bottom_up_act = torch.mm(bottom_input.detach(), layer['bottom_up']) if i == 0 else torch.mm(\n",
    "                self.activations[i-1], layer['bottom_up'])\n",
    "            top_down_act = torch.mm(top_input.detach(), layer['top_down']) if i == self.num_layers - 1 else torch.mm(\n",
    "                self.activations[i+1], layer['top_down'])\n",
    "            recurrent_act = torch.mm(self.activations[i], layer['recurrent'])\n",
    "\n",
    "            total_input = bottom_up_act + top_down_act + recurrent_act\n",
    "            self.activations[i] = F.leaky_relu(total_input)\n",
    "\n",
    "        loss = self.compute_energy()\n",
    "        loss.backward()\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.optimizers[i]['bottom_up'].step()\n",
    "            self.optimizers[i]['top_down'].step()\n",
    "            self.optimizers[i]['recurrent'].step()\n",
    "        \n",
    "        for i in range(0, len(self.activations)):\n",
    "            self.activations[i] = self.activations[i].detach()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_energy(self):\n",
    "        # Push energy down proportional to activations\n",
    "        running_sum = 0\n",
    "        for act in self.activations:\n",
    "            running_sum += torch.mean(act.pow(2))\n",
    "        standard_loss = running_sum / len(self.activations)\n",
    "\n",
    "        # Hebbian loss computation: encourage variance at neuron level\n",
    "        hebbian_loss = 0\n",
    "        for act in self.activations:\n",
    "            hebbian_loss += self.generate_lpl_loss_hebbian(act)\n",
    "\n",
    "        # TODO: predictive and decorrelative losses\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = standard_loss + hebbian_loss  # Consider weighting factors if necessary\n",
    "        # total_loss = standard_loss\n",
    "        return total_loss\n",
    "\n",
    "    def generate_lpl_loss_hebbian(self, activations):\n",
    "        mean_act = torch.mean(activations, dim=0)\n",
    "        mean_subtracted = activations - mean_act\n",
    "        sigma_squared = torch.sum(mean_subtracted ** 2, dim=0) / (activations.shape[0] - 1)\n",
    "        loss = -torch.log(sigma_squared + 1e-10).sum() / sigma_squared.shape[0]\n",
    "        return loss\n",
    "\n",
    "dataset = InputPairsDataset(num_samples=100, input_dim=INPUT_DIM)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Example usage:\n",
    "model = LayerLocalNetwork(bottom_dim=INPUT_DIM, top_dim=INPUT_DIM, num_layers=NUM_LAYERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     for bottom_input, top_input, next_input in dataloader:\n",
    "#         for i in range(ITERATIONS):\n",
    "#             energy = model(bottom_input, top_input)\n",
    "#             # layer_activations = torch.stack([layer_activations.clone() for layer_activations in model.activations], dim=1).reshape(-1, INPUT_DIM)\n",
    "#             print(\"Energy:\", f\"{energy.item(): .2f}\")\n",
    "#         print(\"-----\")\n",
    "#     print()\n",
    "\n",
    "# print(\"======TRYING NEGATIVE SAMPLES======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Average Loss:  37348970.178\n",
      "----\n",
      "Average Loss:  2.588\n",
      "----\n",
      "Average Loss:  4.191\n",
      "----\n",
      "Average Loss:  3.105\n",
      "----\n",
      "Average Loss:  2.027\n",
      "----\n",
      "Average Loss:  2.493\n",
      "----\n",
      "Average Loss:  2.772\n",
      "----\n",
      "Average Loss:  3.263\n",
      "----\n",
      "Average Loss:  2.918\n",
      "----\n",
      "Average Loss:  4.164\n",
      "----\n",
      "Average Loss:  3.597\n",
      "----\n",
      "Average Loss:  3.548\n",
      "----\n",
      "Average Loss:  2.545\n",
      "----\n",
      "Average Loss:  2.290\n",
      "----\n",
      "Average Loss:  2.583\n",
      "----\n",
      "Average Loss:  2.262\n",
      "----\n",
      "Average Loss:  3.992\n",
      "----\n",
      "Average Loss:  2.654\n",
      "----\n",
      "Average Loss:  2.336\n",
      "----\n",
      "Average Loss:  3.374\n",
      "----\n",
      "\n",
      "======TRYING NEGATIVE SAMPLES======\n",
      "Loss:  5.32\n",
      "Loss:  7.13\n",
      "Loss:  7.43\n",
      "Loss:  8.13\n",
      "Loss:  9.27\n",
      "Loss:  11.14\n",
      "Loss:  12.44\n",
      "Loss:  13.96\n",
      "Loss:  14.85\n",
      "Loss:  15.57\n",
      "Loss:  16.19\n",
      "Loss:  16.74\n",
      "Loss:  17.26\n",
      "Loss:  17.76\n",
      "Loss:  18.24\n",
      "Loss:  18.70\n",
      "Loss:  19.14\n",
      "Loss:  19.55\n",
      "Loss:  19.94\n",
      "Loss:  20.29\n",
      "Loss:  20.61\n",
      "Loss:  20.90\n",
      "Loss:  21.15\n",
      "Loss:  21.37\n",
      "Loss:  21.56\n",
      "Loss:  21.72\n",
      "Loss:  21.87\n",
      "Loss:  22.00\n",
      "Loss:  22.12\n",
      "Loss:  22.23\n",
      "Loss:  22.33\n",
      "Loss:  22.43\n",
      "Loss:  22.51\n",
      "Loss:  22.59\n",
      "Loss:  22.67\n",
      "Loss:  22.73\n",
      "Loss:  22.79\n",
      "Loss:  22.84\n",
      "Loss:  22.89\n",
      "Loss:  22.93\n",
      "Loss:  22.96\n",
      "Loss:  22.99\n",
      "Loss:  23.01\n",
      "Loss:  23.02\n",
      "Loss:  23.03\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Loss:  23.04\n",
      "Average Loss:  20.500\n",
      "======TRYING POSITIVE SAMPLE AGAIN======\n",
      "Epoch: 0\n",
      "Average Loss:  2.546\n",
      "----\n",
      "Average Loss:  4.103\n",
      "----\n",
      "Average Loss:  4.801\n",
      "----\n",
      "Average Loss:  3.936\n",
      "----\n",
      "Average Loss:  2.709\n",
      "----\n",
      "Average Loss:  2.449\n",
      "----\n",
      "Average Loss:  3.556\n",
      "----\n",
      "Average Loss:  2.901\n",
      "----\n",
      "Average Loss:  2.767\n",
      "----\n",
      "Average Loss:  2.487\n",
      "----\n",
      "Average Loss:  3.115\n",
      "----\n",
      "Average Loss:  2.412\n",
      "----\n",
      "Average Loss:  1.415\n",
      "----\n",
      "Average Loss:  1.920\n",
      "----\n",
      "Average Loss:  1.559\n",
      "----\n",
      "Average Loss:  1.355\n",
      "----\n",
      "Average Loss:  2.613\n",
      "----\n",
      "Average Loss:  5.062\n",
      "----\n",
      "Average Loss:  3.880\n",
      "----\n",
      "Average Loss:  2.640\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n",
    "\n",
    "print(\"======TRYING NEGATIVE SAMPLES======\")\n",
    "\n",
    "\n",
    "bottom_input = torch.eye(10)[0].reshape(1, -1)  # One-hot vector for bottom input\n",
    "top_input = torch.eye(10)[1].reshape(1, -1)    # One-hot vector for top input\n",
    "\n",
    "running_sum = 0\n",
    "for i in range(75):\n",
    "    loss = model(bottom_input, top_input)\n",
    "    running_sum += loss.item()\n",
    "    print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "print(\"Average Loss:\", f\"{running_sum / 75: .3f}\")\n",
    "\n",
    "\n",
    "print(\"======TRYING POSITIVE SAMPLE AGAIN======\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for bottom_input, top_input, _ in dataloader:\n",
    "        running_sum = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            loss = model(bottom_input, top_input)\n",
    "            running_sum += loss.item()\n",
    "            # print(\"Loss:\", f\"{loss.item(): .2f}\")\n",
    "        print(\"Average Loss:\", f\"{running_sum / ITERATIONS: .3f}\")\n",
    "        # print(\"----\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
